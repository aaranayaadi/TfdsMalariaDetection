# -*- coding: utf-8 -*-
"""Malaria diagnosis CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10KS2bA6e0y0FdrU39dVGpx3uEYSzPwKZ
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Layer, Conv2D, MaxPool2D, Dense, Flatten, InputLayer, BatchNormalization
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import BinaryAccuracy
from google.colab import drive

dataset, dataset_info = tfds.load("malaria", with_info = True, split = ['train'], shuffle_files = True)

dataset

dataset_info

def splits(dataset, TRAIN_RATIO, VAL_RATIO, TEST_RATIO):
  DATASET_SIZE = len(dataset)
  train_ds = dataset.take(int(TRAIN_RATIO * DATASET_SIZE))
  val_test_ds = dataset.skip(int(TRAIN_RATIO * DATASET_SIZE))
  val_ds = val_test_ds.take(int(VAL_RATIO * DATASET_SIZE))
  test_ds = val_test_ds.skip(int(VAL_RATIO * DATASET_SIZE))
  return train_ds, val_ds, test_ds

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1
train_ds, val_ds, test_ds = splits(dataset[0], TRAIN_RATIO, VAL_RATIO, TEST_RATIO)

print(len(train_ds))
print(len(val_ds))
print(len(test_ds))

"""# Data visualisation"""

def get_label_str(label):
  if label == 0:
    return "Uninfected"
  else:
    return "Infected"

for i, sample in enumerate(train_ds.take(16)):
  ax = plt.subplot(4, 4, i+1)
  print(sample['label'].numpy())
  plt.imshow(sample['image'])
  plt.title(get_label_str(sample['label'].numpy()))

"""# Data processing (resizing and normalisation)"""

IM_SIZE = 224
BATCH_SIZE = 32 #creating batches to avoid input size mismatch while training

def resize_rescale(inputs):
  return tf.image.resize(inputs['image'], (IM_SIZE, IM_SIZE))/255.0, inputs['label']

train_dataset = train_ds.map(resize_rescale).shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_dataset = val_ds.map(resize_rescale).shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_dataset = test_ds.map(resize_rescale).shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

for image, label in train_dataset.take(1):
  print(image, label)

np.unique(image)

"""# Building the model (ConvNet)"""

model = tf.keras.Sequential([
    InputLayer(input_shape = (IM_SIZE, IM_SIZE, 3), name='image'),

    tf.keras.layers.Conv2D(filters = 6, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu'), #kernel size is the heightxwidth of the filter. Smaller size helps in locating local features. Larger in global
    BatchNormalization(), #normalises each batch for faster training
    MaxPool2D(pool_size = 2, strides = 2), #reduces spatial dimensions of the input volume

    tf.keras.layers.Conv2D(filters = 16, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu'),
    BatchNormalization(),
    MaxPool2D(pool_size = 2, strides = 2),

    Flatten(),
    Dense(units = 100, activation = 'relu'),
    BatchNormalization(),
    Dense(units = 10, activation = 'relu'),
    BatchNormalization(),
    Dense(units = 1, activation = 'sigmoid')
])
model.summary()

model.compile(
    optimizer = Adam(learning_rate = 0.01),
    loss = BinaryCrossentropy(),
    metrics = BinaryAccuracy()
)

history = model.fit(
    train_dataset,
    validation_data = val_dataset,
    epochs = 100,
    verbose = 1
)

model.evaluate(test_dataset)

def parasite_or_not(x):
  if x<0.5:
    return str("Not infected")
  else:
    return str("Infected")

print(parasite_or_not(model.predict(test_dataset.take(1))[0][0]))

for image, label in test_dataset.take(1):
  for i in range (9):
    ax = plt.subplot(3,3, i+1)
    plt.imshow(image[i])
    plt.title(parasite_or_not(label.numpy()[i])+"-"+parasite_or_not(model.predict(image)[i][0]))
    plt.axis('off')

"""# Loading and Saving

Saving the model
"""

model.save("lenet")

lenet_loaded_model = tf.keras.models.load_model("lenet")
lenet_loaded_model.summary()

model.save("lenet.hdf5")

"""Saving just the weights"""

model.save_weights("weights/lenet_weights")

lenet_weights_model = model.load_weights("weights/lenet_weights")

#to check if the model is running, just run the block where model is built, then compile and then directly evaluate
#no need for training again as the model has been saved and just running the lenet_loaded_model initialises it

"""#Saving to google drive"""

drive.mount("/content/drive/")

!cp -r /content/lenet/ /content/drive/MyDrive/lenet_colab/ #copying the trained lenet model to drive

!cp -r /content/drive/MyDrive/lenet_colab/ /content/lenet_colab/ #copying the trained lenet model from drive to colab when running the model again